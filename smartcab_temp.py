import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
Q = dict()
valid_actions = [None, 'forward', 'left', 'right']  
learning = True
epsilon = 0.5
       
def build_state():
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = 'forward' # The next waypoint 
        right = None
        left = None
        oncoming = None
        light = 'red'

        ########### 
        ## TO DO ##
        ###########
        
        # NOTE : you are not allowed to engineer eatures outside of the inputs available.
        # Because the aim of this project is to teach Reinforcement Learning, we have placed 
        # constraints in order for you to learn how to adjust epsilon and alpha, and thus learn about the balance between exploration and exploitation.
        # With the hand-engineered features, this learning process gets entirely negated.
        
        # Set 'state' as a tuple of relevant data for the agent        
        state = (("directions", waypoint),("right", right),("left", left),("onccoming", oncoming), ("light", light))

        return state

def createQ(state):
        """ The createQ function is called when a state is generated by the agent. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0

        
        action = {}
        for action_item in valid_actions:
            action[action_item] = 0.0
            
        if learning == True:
            if Q.has_key(state) == False:
                Q[state] = action
                    

        return
    
def get_maxQ(state):
    """ The get_max_Q function is called when the agent is asked to find the
        maximum Q-value of all actions based on the 'state' the smartcab is in. """

    ########### 
    ## TO DO ##
    ###########
    # Calculate the maximum Q-value of all actions for a given state

    maxQ = None
    max_value = 0
    
    for item in Q[state]:
        #print 'Dict',self.Q[state][item]
        if Q[state][item] > max_value:
            max_value = Q[state][item]
            maxQ = item
            
    return maxQ 
    
def choose_action(state):
    """ The choose_action function is called when the agent is asked to choose
        which action to take, based on the 'state' the smartcab is in. """

    # Set the agent state and default action
    state = state
    #next_waypoint = self.planner.next_waypoint()
    action = 0

    ########### 
    ## TO DO ##
    ###########
    # When not learning, choose a random action
    # When learning, choose a random action with 'epsilon' probability
    # Otherwise, choose an action with the highest Q-value for the current state
    # Be sure that when choosing an action with highest Q-value that you randomly select between actions that "tie".
    
    action_list = []
        
   
    maxVal = get_maxQ(state)
    for item in Q[state]:
        if Q[state][item] == maxVal:
            action_list + item
    
        action = random.choice(action_list)
        print action
        print maxVal
        return action

state = build_state()          # Get current state
createQ(state)
print choose_action(state)

s = random.random()
    
       

